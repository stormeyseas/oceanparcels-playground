{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76962116",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#%reset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt, animation\n",
    "import cartopy\n",
    "import datetime as dati\n",
    "import netCDF4 as ntc\n",
    "from operator import attrgetter\n",
    "import time as tm\n",
    "import os\n",
    "from os import chdir, getcwd\n",
    "import math\n",
    "from math import sqrt, pi, cos\n",
    "import warnings as wr\n",
    "\n",
    "scen = np.array([\"S0\", \"S1\", \"S2\", \"S3\", \"S4\", \"S5\", \"S6\"])\n",
    "mon = np.array([\"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0036f701",
   "metadata": {},
   "source": [
    "# Rechunk and save data\n",
    "\n",
    "May, July, August, and October (31 days) should have 6912 trajectories, or 54 psets. June and Septermber should have 6656 trajectories, or 52 psets. The number of observations will depend on particle residence time.\n",
    "\n",
    "In the data, 'trajectory' is particles, and 'obs' (observations) is times relative to the particle - so every particle will have an obs=0 but only a few will have the maximum number of observations. On creation, each chunk in the data is all particles at one observation. The rechunking reorganises into chunks of 24 time records (24 hours) and 128 particles (1 pset)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a0e2f6",
   "metadata": {},
   "source": [
    "## All in one go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da617674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting D:\\OceanParcels\\outputs2\\S0_may.zarr\n",
      "Starting D:\\OceanParcels\\outputs2\\S1_may.zarr\n",
      "Starting D:\\OceanParcels\\outputs2\\S2_may.zarr\n",
      "Starting D:\\OceanParcels\\outputs2\\S0_jun.zarr\n",
      "Starting D:\\OceanParcels\\outputs2\\S1_jun.zarr\n",
      "Starting D:\\OceanParcels\\outputs2\\S2_jun.zarr\n",
      "Starting D:\\OceanParcels\\outputs2\\S0_jul.zarr\n",
      "Starting D:\\OceanParcels\\outputs2\\S1_jul.zarr\n",
      "Starting D:\\OceanParcels\\outputs2\\S2_jul.zarr\n",
      "Starting D:\\OceanParcels\\outputs2\\S0_aug.zarr\n",
      "Starting D:\\OceanParcels\\outputs2\\S1_aug.zarr\n",
      "Starting D:\\OceanParcels\\outputs2\\S2_aug.zarr\n",
      "Starting D:\\OceanParcels\\outputs2\\S0_sep.zarr\n",
      "Starting D:\\OceanParcels\\outputs2\\S1_sep.zarr\n",
      "Starting D:\\OceanParcels\\outputs2\\S2_sep.zarr\n",
      "Starting D:\\OceanParcels\\outputs2\\S0_oct.zarr\n",
      "Starting D:\\OceanParcels\\outputs2\\S1_oct.zarr\n",
      "Starting D:\\OceanParcels\\outputs2\\S2_oct.zarr\n"
     ]
    }
   ],
   "source": [
    "#%%script echo Skipped!\n",
    "\n",
    "#traj_n = np.array([6912, 6656, 6912, 6912, 6656, 6912])\n",
    "\n",
    "for mo in range(6):\n",
    "    for sc in [0,1,2]:\n",
    "        path = \"D:\\\\OceanParcels\\\\\"\n",
    "        zarr2 =  path + \"outputs2\\\\\" + scen[sc] + \"_\" + mon[mo] + \".zarr\"\n",
    "        nc2 = path + \"outputs2\\\\\" + scen[sc] + \"_\" + mon[mo] + \"_set2.nc\"\n",
    "        print(\"Starting \" + zarr2)\n",
    "        \n",
    "        # Open new runfile\n",
    "        ds = xr.open_zarr(zarr2, decode_times = False).load()\n",
    "        ds.close()      \n",
    "        ds = ds.drop_vars('trajectory', errors = 'ignore')\n",
    "        \n",
    "        # Check there are enough psets\n",
    "        traj1 = sum(np.shape(ds.trajectory.values))\n",
    "        \n",
    "        #if traj1 != traj_n[mo]:\n",
    "        #    print(\"Not enough psets!\")\n",
    "        #    break\n",
    "        \n",
    "        lats = ds.lat.values\n",
    "        lons = ds.lon.values\n",
    "        deps = ds.z.values\n",
    "\n",
    "        nparts = np.shape(lats)[0]\n",
    "\n",
    "        lat_dist = np.absolute(np.diff(lats, axis=1))\n",
    "        lat_dist = np.concatenate([np.zeros(shape = (nparts, 1)), lat_dist], axis = 1) * 1110 # in m\n",
    "        lon_dist = np.absolute(np.diff(lons, axis=1))\n",
    "        lon_dist = np.concatenate([np.zeros(shape = (nparts, 1)), lon_dist], axis = 1) * 1110 * np.cos(lats * pi/180)\n",
    "        dep_dist = np.absolute(np.diff(deps, axis=1))\n",
    "        dep_dist = np.concatenate([np.zeros(shape = (nparts, 1)), dep_dist], axis = 1)\n",
    "        dist_2D = np.sqrt(np.power(lat_dist, 2) + np.power(lon_dist, 2))\n",
    "        dist_3D = np.sqrt(np.power(lat_dist, 2) + np.power(lon_dist, 2) + np.power(dep_dist, 2))\n",
    "\n",
    "        ds_dist = xr.Dataset(data_vars = {'lat_dist': (['trajectory','obs'], lat_dist),\n",
    "                                          'lon_dist': (['trajectory','obs'], lon_dist),\n",
    "                                          'dep_dist': (['trajectory','obs'], dep_dist), \n",
    "                                          'dist_2D': (['trajectory','obs'], dist_2D),\n",
    "                                          'dist_3D': (['trajectory','obs'], dist_3D)})\n",
    "\n",
    "        ds = ds.merge(ds_dist)\n",
    "        \n",
    "        ds.obs.attrs = {'long_name': 'Nth observation of particle', 'standard_name': 'observation'}\n",
    "        ds.z.attrs = {'long_name': 'Position in water column', 'units': 'm', 'positive': 'down', 'standard_name': 'depth'}\n",
    "        ds.lat.attrs = {'long_name': 'Degrees latitude', 'units': 'degrees_north', 'standard_name': 'latitude'}\n",
    "        ds.lon.attrs = {'long_name': 'Degrees longitude', 'units': 'degrees_east', 'standard_name': 'longitude'}\n",
    "        ds.time.attrs = {'long_name': 'Fieldset time', 'standard_name': 'time'}\n",
    "        ds_dist.lat_dist.attrs = {'long_name': 'Distance travelled in latitudinal direction', 'unit': 'm', 'standard_name': 'lat_dist'}\n",
    "        ds_dist.lon_dist.attrs = {'long_name': 'Distance travelled in longitudinal direction', 'unit': 'm', 'standard_name': 'lon_dist'}\n",
    "        ds_dist.dep_dist.attrs = {'long_name': 'Distance travelled in vertical direction', 'unit': 'm', 'standard_name': 'dep_dist'}\n",
    "        ds_dist.dist_2D.attrs = {'long_name': 'Horizontal distance travelled', 'unit': 'm', 'standard_name': 'dist_2D'}\n",
    "        ds_dist.dist_3D.attrs = {'long_name': 'Horizontal and vertical distance travelled', 'unit': 'm', 'standard_name': 'dist_3D'}\n",
    "        xr.set_options(keep_attrs = True)\n",
    "        \n",
    "        ds_rechunked = ds.chunk({\"trajectory\": 64, \"obs\": 24})       \n",
    "        ds_rechunked.to_netcdf(nc2)\n",
    "        \n",
    "        del ds, ds_rechunked\n",
    "\n",
    "# ds_rechunked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d049ee",
   "metadata": {},
   "source": [
    "## Remove duplicates from first set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5ea4551",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mo in range(6):\n",
    "    for sc in range(7):\n",
    "        path = \"D:\\\\OceanParcels\\\\\"\n",
    "        nc1 = path + \"outputs\\\\\" + scen[sc] + \"_\" + mon[mo] + \".nc\"\n",
    "        nc2 = path + \"outputs2\\\\\" + scen[sc] + \"_\" + mon[mo] + \"_set1.nc\"\n",
    "        \n",
    "        # Open original runfile\n",
    "        ds1 = xr.open_dataset(nc1, decode_times = False).load()\n",
    "        ds1 = ds1.drop_vars('trajectory', errors = 'ignore')\n",
    "        ds1.close()\n",
    "        \n",
    "        # Remove duplicates\n",
    "        tot = sum(np.shape(ds1.trajectory.values))\n",
    "        done = np.arange(0, tot, step = 2)\n",
    "        ds1 = ds1.sel(trajectory = done)\n",
    "        \n",
    "        ds_rechunked = ds1.chunk({\"trajectory\": 64, \"obs\": 24}) \n",
    "        ds_rechunked.to_netcdf(nc2)\n",
    "        \n",
    "        del ds1, ds_rechunked\n",
    "        # Merge the two together? (does this work??)\n",
    "        #ds_rechunked = ds_rechunked.merge(ds1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb61d5b7",
   "metadata": {},
   "source": [
    "## Seperated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a1c59e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped!\n"
     ]
    }
   ],
   "source": [
    "%%script echo Skipped!\n",
    "\n",
    "mo = 0\n",
    "sc = 3\n",
    "\n",
    "# For working on home computer\n",
    "path = \"D:\\\\OceanParcels\\\\outputs\\\\\"\n",
    "\n",
    "scen = np.array([\"S0\", \"S1\", \"S2\", \"S3\", \"S4\", \"S5\", \"S6\"])\n",
    "mon = np.array([\"may\", \"jun\", \"jul\", \"aug\", \"sep\", \"oct\"])\n",
    "\n",
    "zarrname = path + \"backup zarr files (spujb only)\\\\\" + scen[sc] + \" zarrs\\\\atten_\" + mon[mo] + \"_\" + scen[sc] + \".zarr\"\n",
    "ncname = path + scen[sc] + \"_\" + mon[mo] + \".nc\"\n",
    "print(zarrname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "18f1d75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped!\n"
     ]
    }
   ],
   "source": [
    "%%script echo Skipped!\n",
    "\n",
    "ds = xr.open_zarr(zarrname).load()\n",
    "ds.close()\n",
    "ds # just checking it looks ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ca2bd18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped!\n"
     ]
    }
   ],
   "source": [
    "%%script echo Skipped!\n",
    "\n",
    "lats = ds.lat.values\n",
    "lons = ds.lon.values\n",
    "deps = ds.z.values\n",
    "\n",
    "nparts = np.shape(lats)[0]\n",
    "\n",
    "lat_dist = np.absolute(np.diff(lats, axis=1))\n",
    "lat_dist = np.concatenate([np.zeros(shape = (nparts, 1)), lat_dist], axis = 1) * 1110 # in m\n",
    "lon_dist = np.absolute(np.diff(lons, axis=1))\n",
    "lon_dist = np.concatenate([np.zeros(shape = (nparts, 1)), lon_dist], axis = 1) * 1110 * np.cos(lats * pi/180)\n",
    "dep_dist = np.absolute(np.diff(deps, axis=1))\n",
    "dep_dist = np.concatenate([np.zeros(shape = (nparts, 1)), dep_dist], axis = 1)\n",
    "dist_2D = np.sqrt(np.power(lat_dist, 2) + np.power(lon_dist, 2))\n",
    "dist_3D = np.sqrt(np.power(lat_dist, 2) + np.power(lon_dist, 2) + np.power(dep_dist, 2))\n",
    "\n",
    "ds_dist = xr.Dataset(data_vars = {'lat_dist': (['trajectory','obs'], lat_dist),\n",
    "                                  'lon_dist': (['trajectory','obs'], lon_dist),\n",
    "                                  'dep_dist': (['trajectory','obs'], dep_dist), \n",
    "                                  'dist_2D': (['trajectory','obs'], dist_2D),\n",
    "                                  'dist_3D': (['trajectory','obs'], dist_3D)})\n",
    "\n",
    "ds = ds.merge(ds_dist).drop_vars('trajectory')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4f1e98d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped!\n"
     ]
    }
   ],
   "source": [
    "%%script echo Skipped!\n",
    "\n",
    "ds.obs.attrs = {'long_name': 'Nth observation of particle', 'standard_name': 'observation'}\n",
    "# ds.trajectory.attrs = {'long_name': 'Particle ID', 'standard_name': 'particle'}\n",
    "ds.z.attrs = {'long_name': 'Position in water column', 'units': 'm', 'positive': 'down', 'standard_name': 'depth'}\n",
    "ds.lat.attrs = {'long_name': 'Degrees latitude', 'units': 'degrees_north', 'standard_name': 'latitude'}\n",
    "ds.lon.attrs = {'long_name': 'Degrees longitude', 'units': 'degrees_east', 'standard_name': 'longitude'}\n",
    "ds.time.attrs = {'long_name': 'Fieldset time', 'standard_name': 'time'}\n",
    "ds_dist.lat_dist.attrs = {'long_name': 'Distance travelled in latitudinal direction', 'unit': 'm', 'standard_name': 'lat_dist'}\n",
    "ds_dist.lon_dist.attrs = {'long_name': 'Distance travelled in longitudinal direction', 'unit': 'm', 'standard_name': 'lon_dist'}\n",
    "ds_dist.dep_dist.attrs = {'long_name': 'Distance travelled in vertical direction', 'unit': 'm', 'standard_name': 'dep_dist'}\n",
    "ds_dist.dist_2D.attrs = {'long_name': 'Horizontal distance travelled', 'unit': 'm', 'standard_name': 'dist_2D'}\n",
    "ds_dist.dist_3D.attrs = {'long_name': 'Horizontal and vertical distance travelled', 'unit': 'm', 'standard_name': 'dist_3D'}\n",
    "xr.set_options(keep_attrs = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "aa9d8504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped!\n"
     ]
    }
   ],
   "source": [
    "%%script echo Skipped!\n",
    "\n",
    "ds_rechunked = ds.chunk({\"trajectory\": 128, \"obs\": 24})\n",
    "display(ds_rechunked.chunks)\n",
    "ds_rechunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f2ef4afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped!\n"
     ]
    }
   ],
   "source": [
    "%%script echo Skipped!\n",
    "\n",
    "ds_rechunked.to_netcdf(ncname)\n",
    "del ds, ds_rechunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da21ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
